{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE-20ulmtGvf"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbWg8KK_tH5R"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers[torch] tensorboardx simpletransformers\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNMPVGE1tJRG"
      },
      "outputs": [],
      "source": [
        "from simpletransformers.classification import ClassificationModel\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4JngVRX1unPn"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ry22QmhtK3A"
      },
      "outputs": [],
      "source": [
        "file_id = \"1XOafk3wcP2RcTu1MHXoR_IJBIZseqTn8\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "train_model_df = pd.read_csv(url, sep=\"\\t\")\n",
        "train_model_df = train_model_df.loc[:, ~train_model_df.columns.str.contains('^Unnamed')]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1K58n9-BvhAe"
      },
      "source": [
        "# Split Train and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ru6t-yUGtMpw"
      },
      "outputs": [],
      "source": [
        "train_df, val_df, _, _ = train_test_split(\n",
        "    train_model_df,\n",
        "    train_model_df[\"target_flag\"],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=train_model_df[\"target_flag\"]\n",
        ")\n",
        "\n",
        "print(f\"train size: {len(train_df)}, target_rate: {train_df.agg({'target_flag': 'mean'})}\")\n",
        "print(f\"val size: {len(val_df)}, target_rate: {val_df.agg({'target_flag': 'mean'})}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnNfcOxmtOKJ"
      },
      "outputs": [],
      "source": [
        "selected_cols = [\"keyword\", \"text\", \"target_flag\"]\n",
        "train_df = train_df[selected_cols]\n",
        "val_df = val_df[selected_cols]\n",
        "\n",
        "train_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ppbj0bB9vkMP"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgP6znHOtUYH"
      },
      "outputs": [],
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    else:\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "def plot_f1(train_f1_list, val_f1_list):\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(np.arange(len(train_f1_list)), train_f1_list, label=\"train\")\n",
        "    ax.plot(np.arange(len(val_f1_list)), val_f1_list, label=\"val\")\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "def augment_text(row, deletion_prob=0.0, swap_prob=0.7, pos=3):\n",
        "    # Tokenize the text\n",
        "    tokens = row['text'].split()\n",
        "\n",
        "    n_pos = 1*pos\n",
        "\n",
        "    for i in range(len(tokens)-n_pos):\n",
        "        if random.random() < swap_prob:\n",
        "            #swap_i = random.randint(0, len(tokens)-1)\n",
        "            tokens[i], tokens[i + n_pos] = tokens[i + n_pos], tokens[i]\n",
        "\n",
        "    tokens = [token for token in tokens if random.random() > deletion_prob]\n",
        "\n",
        "    # Reconstruct the augmented text\n",
        "    augmented_text = ' '.join(tokens)\n",
        "    return augmented_text\n",
        "\n",
        "def train_model(train_df, val_df, custom_args, cols=['text', 'target_flag'], epochs=3, is_save=True, is_swap=False, start_swap_epoch=0):\n",
        "\n",
        "    save_path = f'/models/{str(custom_args[\"learning_rate\"]):.4}_{str(custom_args[\"weight_decay\"]):.4}'\n",
        "\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    # Create a ClassificationModel with custom hyperparameters\n",
        "    model = ClassificationModel(\n",
        "        \"distilbert\",\n",
        "        \"distilbert-base-uncased\",\n",
        "        num_labels=2,\n",
        "        args=custom_args\n",
        "    )\n",
        "\n",
        "    train_f1_list = []\n",
        "    val_f1_list = []\n",
        "\n",
        "    best_f1 = -1\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "        _train_df = train_df.copy()\n",
        "        if is_swap and i >= start_swap_epoch:\n",
        "            # _train_df[\"text\"] =  _train_df.apply(augment_text, axis=1)\n",
        "            _train_df[\"text\"] = _train_df.apply(lambda row: augment_text(row, pos=i+1), axis=1)\n",
        "\n",
        "        model.train_model(_train_df[cols], eval_df=val_df[cols])\n",
        "\n",
        "        train_result, train_model_outputs, train_wrong_predictions = model.eval_model(_train_df[cols])\n",
        "        val_result, val_model_outputs, val_wrong_predictions = model.eval_model(val_df[cols])\n",
        "        train_f1_list.append(train_result[\"f1_score\"])\n",
        "        val_f1_list.append(val_result[\"f1_score\"])\n",
        "\n",
        "        if val_result[\"f1_score\"] > best_f1:\n",
        "            best_f1 = val_result[\"f1_score\"]\n",
        "            if is_save:\n",
        "                model.model.save_pretrained(save_path)\n",
        "                model.tokenizer.save_pretrained(save_path)\n",
        "                model.config.save_pretrained(f'{save_path}/')\n",
        "                if not os.path.isfile(os.path.join(save_path, 'config.json')):\n",
        "                    raise Exception(\"Model not saved correctly. 'config.json' not found.\")\n",
        "        else:\n",
        "            print(f\"Early stop at: {i}\")\n",
        "            break\n",
        "\n",
        "        print(f\"Epoch {i} train: {train_result['f1_score']}, val: {val_result['f1_score']}\")\n",
        "\n",
        "    if is_save:\n",
        "        best_model = ClassificationModel(\n",
        "            \"distilbert\",\n",
        "            save_path,\n",
        "            # num_labels=2,  # Ensure this matches the original model's configuration\n",
        "        )\n",
        "    else:\n",
        "        best_model = model\n",
        "\n",
        "    train_result, train_model_outputs, train_wrong_predictions = best_model.eval_model(_train_df[cols])\n",
        "    val_result, val_model_outputs, val_wrong_predictions = best_model.eval_model(val_df[cols])\n",
        "\n",
        "    return best_model, train_result[\"f1_score\"], val_result[\"f1_score\"], train_f1_list, val_f1_list"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e9-BCaBgvmf9"
      },
      "source": [
        "# Paraphrase by T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFgfZ2QWtYOG"
      },
      "outputs": [],
      "source": [
        "target_one_text = train_df.loc[train_df[\"target_flag\"] == 1.0, \"text\"].tolist()\n",
        "\n",
        "device = get_device()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\").to(device)\n",
        "\n",
        "\n",
        "paraphrase_list = []\n",
        "\n",
        "for i, sentence in tqdm(enumerate(target_one_text)):\n",
        "\n",
        "    text =  \"paraphrase: \" + sentence + \" </s>\"\n",
        "\n",
        "    encoding = tokenizer.encode_plus(text, pad_to_max_length=True, return_tensors=\"pt\")\n",
        "    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids, attention_mask=attention_masks,\n",
        "        max_length=256,\n",
        "        do_sample=True,\n",
        "        top_k=240,\n",
        "        top_p=0.99,\n",
        "        early_stopping=True,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "    for output in outputs:\n",
        "        line = tokenizer.decode(output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "        paraphrase_list.append(line)\n",
        "\n",
        "    torch.mps.empty_cache()\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPGUCju3uA6-"
      },
      "outputs": [],
      "source": [
        "paraphrase_dict = {\n",
        "    \"text\": paraphrase_list,\n",
        "    \"target_flag\": 1.0,\n",
        "}\n",
        "\n",
        "paraphrase_df_1 = pd.DataFrame(paraphrase_dict)\n",
        "train_all_df = pd.concat([paraphrase_df_1, train_df])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OKvKptYAvt5e"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRqVfgU8tdn-"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "best_params = {\n",
        "    \"learning_rate\": 3e-5,\n",
        "    \"train_batch_size\": batch_size,\n",
        "    \"eval_batch_size\": batch_size,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"optimizer\": \"AdamW\",\n",
        "    \"num_train_epochs\": 1,\n",
        "    \"dropout_rate\": 0.1,\n",
        "    \"overwrite_output_dir\": True,\n",
        "}\n",
        "\n",
        "cols = ['text', 'target_flag']\n",
        "\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "model, train_f1, val_f1, train_f1_list, val_f1_list = train_model(\n",
        "    train_all_df,\n",
        "    val_df,\n",
        "    best_params,\n",
        "    cols=cols,\n",
        "    epochs=5,\n",
        "    is_save=True,\n",
        "    is_swap=True,\n",
        "    start_swap_epoch=1\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
